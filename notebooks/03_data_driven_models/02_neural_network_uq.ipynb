{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network UQ Methods: NNGMM and NNBR\n",
    "\n",
    "This notebook demonstrates two neural network-based uncertainty quantification methods:\n",
    "\n",
    "1. **NNGMM**: Neural Network + Gaussian Mixture Model\n",
    "2. **NNBR**: Neural Network + Bayesian Linear Regression\n",
    "\n",
    "Both methods use a neural network to extract features from the input, then apply different statistical methods for uncertainty estimation in the learned feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from pycse.sklearn.nngmm import NeuralNetworkGMM\n",
    "from pycse.sklearn.nnbr import NeuralNetworkBLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.datasets.linear import LineDataset\n",
    "from src.datasets.nonlinear import ExponentialDecayDataset\n",
    "from src.visualization import setup_plot_style\n",
    "from src.utils.seeds import set_global_seed\n",
    "\n",
    "# Set up plotting\n",
    "setup_plot_style()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural Network + Gaussian Mixture Model (NNGMM)\n",
    "\n",
    "### How NNGMM Works\n",
    "\n",
    "NNGMM is a two-stage approach:\n",
    "\n",
    "1. **Feature Extraction**: A neural network learns a nonlinear transformation of the input features\n",
    "2. **Uncertainty Estimation**: A Gaussian Mixture Model (GMM) is fit in the learned feature space to estimate prediction uncertainty\n",
    "\n",
    "**Key Idea**: The NN captures complex nonlinear relationships, while the GMM provides a probabilistic model in the transformed space.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Input → NN Hidden Layers → Features → GMM → (mean, std)\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- Can capture multimodal uncertainty distributions\n",
    "- Flexible nonlinear modeling\n",
    "\n",
    "**Challenges**:\n",
    "- GMM fitting can be unstable (may diverge or produce poor fits)\n",
    "- Sensitive to number of GMM components\n",
    "- No built-in calibration mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "set_global_seed(42)\n",
    "\n",
    "# Create a simple dataset with homoskedastic noise\n",
    "dataset = LineDataset(\n",
    "    slope=0.8,\n",
    "    intercept=0.1,\n",
    "    n_samples=100,\n",
    "    noise_model='homoskedastic',\n",
    "    noise_level=0.05,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "data = dataset.generate()\n",
    "\n",
    "# Reshape for sklearn\n",
    "X_train = data.X_train.reshape(-1, 1)\n",
    "y_train = data.y_train\n",
    "X_test = data.X_test.reshape(-1, 1)\n",
    "y_test = data.y_test\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create backend neural network\n",
    "mlp_gmm = MLPRegressor(\n",
    "    hidden_layer_sizes=(20,),\n",
    "    activation='tanh',\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create NNGMM with 3 GMM components\n",
    "nngmm = NeuralNetworkGMM(\n",
    "    nn=mlp_gmm,\n",
    "    n_components=3,\n",
    "    n_samples=500\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "print(\"Fitting NNGMM model...\")\n",
    "nngmm.fit(X_train, y_train)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with uncertainty\n",
    "y_pred_gmm, y_std_gmm = nngmm.predict(X_test, return_std=True)\n",
    "y_pred_gmm = y_pred_gmm.flatten()\n",
    "\n",
    "# Compute 95% prediction intervals\n",
    "z_score = 1.96\n",
    "y_lower_gmm = y_pred_gmm - z_score * y_std_gmm\n",
    "y_upper_gmm = y_pred_gmm + z_score * y_std_gmm\n",
    "\n",
    "# Calculate metrics\n",
    "rmse_gmm = np.sqrt(np.mean((y_test - y_pred_gmm)**2))\n",
    "coverage_gmm = np.mean((y_test >= y_lower_gmm) & (y_test <= y_upper_gmm))\n",
    "mean_width_gmm = np.mean(y_upper_gmm - y_lower_gmm)\n",
    "\n",
    "print(f\"NNGMM Results:\")\n",
    "print(f\"  RMSE: {rmse_gmm:.4f}\")\n",
    "print(f\"  Coverage: {coverage_gmm:.3f} (target: 0.95)\")\n",
    "print(f\"  Mean Width: {mean_width_gmm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize NNGMM predictions\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Sort for plotting\n",
    "sort_idx = np.argsort(X_test.flatten())\n",
    "X_sorted = X_test[sort_idx].flatten()\n",
    "y_pred_sorted = y_pred_gmm[sort_idx]\n",
    "y_lower_sorted = y_lower_gmm[sort_idx]\n",
    "y_upper_sorted = y_upper_gmm[sort_idx]\n",
    "\n",
    "# Plot training data\n",
    "ax.scatter(X_train, y_train, alpha=0.6, s=30, label='Training data', color='blue')\n",
    "\n",
    "# Plot test data\n",
    "ax.scatter(X_test, y_test, alpha=0.3, s=20, label='Test data', color='gray')\n",
    "\n",
    "# Plot predictions\n",
    "ax.plot(X_sorted, y_pred_sorted, 'r-', linewidth=2, label='NNGMM prediction')\n",
    "\n",
    "# Plot uncertainty band\n",
    "ax.fill_between(X_sorted, y_lower_sorted, y_upper_sorted, \n",
    "                 alpha=0.3, color='red', label='95% prediction interval')\n",
    "\n",
    "# Plot true function\n",
    "X_plot = np.linspace(0, 1, 200)\n",
    "y_true = dataset._generate_clean(X_plot)\n",
    "ax.plot(X_plot, y_true, 'k--', linewidth=2, label='True function', zorder=10)\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title(f'NNGMM: Coverage = {coverage_gmm:.3f}, Width = {mean_width_gmm:.4f}')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network + Bayesian Linear Regression (NNBR)\n",
    "\n",
    "### How NNBR Works\n",
    "\n",
    "NNBR is also a two-stage approach:\n",
    "\n",
    "1. **Feature Extraction**: A neural network learns nonlinear features from the input\n",
    "2. **Uncertainty Estimation**: Bayesian Linear Regression is applied in the feature space to estimate uncertainty\n",
    "\n",
    "**Key Idea**: The NN captures complex patterns, while Bayesian regression provides principled uncertainty estimates.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Input → NN Hidden Layers → Features → Bayesian Linear Regression → (mean, std)\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- More stable than GMM fitting\n",
    "- **Post-hoc calibration** using validation data improves coverage\n",
    "- Computationally efficient\n",
    "\n",
    "**Calibration Procedure**:\n",
    "NNBR uses a validation split to calibrate uncertainties:\n",
    "1. Split training data into fit (80%) and validation (20%)\n",
    "2. Train NN and Bayesian regressor on fit data\n",
    "3. Evaluate on validation data to learn a calibration factor\n",
    "4. Apply calibration to scale uncertainties for better coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create backend neural network for NNBR\n",
    "mlp_br = MLPRegressor(\n",
    "    hidden_layer_sizes=(20,),\n",
    "    activation='tanh',\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create Bayesian Ridge regressor\n",
    "br = BayesianRidge()\n",
    "\n",
    "# Create NNBR\n",
    "nnbr = NeuralNetworkBLR(nn=mlp_br, br=br)\n",
    "\n",
    "# Split training data for calibration (80% fit, 20% validation)\n",
    "X_train_fit, X_train_val, y_train_fit, y_train_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Fit samples: {len(X_train_fit)}\")\n",
    "print(f\"Validation samples: {len(X_train_val)}\")\n",
    "\n",
    "# Fit with calibration\n",
    "print(\"\\nFitting NNBR model with calibration...\")\n",
    "nnbr.fit(X_train_fit, y_train_fit, val_X=X_train_val, val_y=y_train_val)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with uncertainty\n",
    "y_pred_br, y_std_br = nnbr.predict(X_test, return_std=True)\n",
    "if y_pred_br.ndim > 1:\n",
    "    y_pred_br = y_pred_br.flatten()\n",
    "\n",
    "# Compute 95% prediction intervals\n",
    "z_score = 1.96\n",
    "y_lower_br = y_pred_br - z_score * y_std_br\n",
    "y_upper_br = y_pred_br + z_score * y_std_br\n",
    "\n",
    "# Calculate metrics\n",
    "rmse_br = np.sqrt(np.mean((y_test - y_pred_br)**2))\n",
    "coverage_br = np.mean((y_test >= y_lower_br) & (y_test <= y_upper_br))\n",
    "mean_width_br = np.mean(y_upper_br - y_lower_br)\n",
    "\n",
    "print(f\"NNBR Results:\")\n",
    "print(f\"  RMSE: {rmse_br:.4f}\")\n",
    "print(f\"  Coverage: {coverage_br:.3f} (target: 0.95)\")\n",
    "print(f\"  Mean Width: {mean_width_br:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize NNBR predictions\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Sort for plotting\n",
    "sort_idx = np.argsort(X_test.flatten())\n",
    "X_sorted = X_test[sort_idx].flatten()\n",
    "y_pred_sorted = y_pred_br[sort_idx]\n",
    "y_lower_sorted = y_lower_br[sort_idx]\n",
    "y_upper_sorted = y_upper_br[sort_idx]\n",
    "\n",
    "# Plot training data\n",
    "ax.scatter(X_train, y_train, alpha=0.6, s=30, label='Training data', color='blue')\n",
    "\n",
    "# Plot test data\n",
    "ax.scatter(X_test, y_test, alpha=0.3, s=20, label='Test data', color='gray')\n",
    "\n",
    "# Plot predictions\n",
    "ax.plot(X_sorted, y_pred_sorted, 'g-', linewidth=2, label='NNBR prediction')\n",
    "\n",
    "# Plot uncertainty band\n",
    "ax.fill_between(X_sorted, y_lower_sorted, y_upper_sorted, \n",
    "                 alpha=0.3, color='green', label='95% prediction interval (calibrated)')\n",
    "\n",
    "# Plot true function\n",
    "X_plot = np.linspace(0, 1, 200)\n",
    "y_true = dataset._generate_clean(X_plot)\n",
    "ax.plot(X_plot, y_true, 'k--', linewidth=2, label='True function', zorder=10)\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title(f'NNBR (Calibrated): Coverage = {coverage_br:.3f}, Width = {mean_width_br:.4f}')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Side-by-Side Comparison\n",
    "\n",
    "Let's compare both methods on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# NNGMM plot\n",
    "ax = axes[0]\n",
    "sort_idx = np.argsort(X_test.flatten())\n",
    "X_sorted = X_test[sort_idx].flatten()\n",
    "\n",
    "ax.scatter(X_train, y_train, alpha=0.6, s=30, label='Training data', color='blue')\n",
    "ax.scatter(X_test, y_test, alpha=0.3, s=20, label='Test data', color='gray')\n",
    "ax.plot(X_sorted, y_pred_gmm[sort_idx], 'r-', linewidth=2, label='Prediction')\n",
    "ax.fill_between(X_sorted, y_lower_gmm[sort_idx], y_upper_gmm[sort_idx], \n",
    "                 alpha=0.3, color='red', label='95% interval')\n",
    "ax.plot(X_plot, y_true, 'k--', linewidth=2, label='True function', zorder=10)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title(f'NNGMM\\nCoverage={coverage_gmm:.3f}, Width={mean_width_gmm:.4f}, RMSE={rmse_gmm:.4f}')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# NNBR plot\n",
    "ax = axes[1]\n",
    "ax.scatter(X_train, y_train, alpha=0.6, s=30, label='Training data', color='blue')\n",
    "ax.scatter(X_test, y_test, alpha=0.3, s=20, label='Test data', color='gray')\n",
    "ax.plot(X_sorted, y_pred_br[sort_idx], 'g-', linewidth=2, label='Prediction')\n",
    "ax.fill_between(X_sorted, y_lower_br[sort_idx], y_upper_br[sort_idx], \n",
    "                 alpha=0.3, color='green', label='95% interval (calibrated)')\n",
    "ax.plot(X_plot, y_true, 'k--', linewidth=2, label='True function', zorder=10)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title(f'NNBR (Calibrated)\\nCoverage={coverage_br:.3f}, Width={mean_width_br:.4f}, RMSE={rmse_br:.4f}')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['NNGMM', 'NNBR'],\n",
    "    'Coverage': [f'{coverage_gmm:.3f}', f'{coverage_br:.3f}'],\n",
    "    'RMSE': [f'{rmse_gmm:.4f}', f'{rmse_br:.4f}'],\n",
    "    'Mean Width': [f'{mean_width_gmm:.4f}', f'{mean_width_br:.4f}']\n",
    "})\n",
    "\n",
    "print(\"\\nMethod Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\nTarget coverage: 0.95\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing on a Nonlinear Dataset\n",
    "\n",
    "Let's test both methods on a more challenging nonlinear dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create exponential decay dataset\n",
    "set_global_seed(42)\n",
    "\n",
    "exp_dataset = ExponentialDecayDataset(\n",
    "    a=2.0,\n",
    "    b=3.0,\n",
    "    c=0.5,\n",
    "    n_samples=100,\n",
    "    noise_model='homoskedastic',\n",
    "    noise_level=0.05,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "exp_data = exp_dataset.generate()\n",
    "\n",
    "X_train_exp = exp_data.X_train.reshape(-1, 1)\n",
    "y_train_exp = exp_data.y_train\n",
    "X_test_exp = exp_data.X_test.reshape(-1, 1)\n",
    "y_test_exp = exp_data.y_test\n",
    "\n",
    "print(f\"Exponential Decay Dataset:\")\n",
    "print(f\"  Training samples: {len(X_train_exp)}\")\n",
    "print(f\"  Test samples: {len(X_test_exp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NNGMM on exponential dataset\n",
    "mlp_gmm_exp = MLPRegressor(\n",
    "    hidden_layer_sizes=(20,),\n",
    "    activation='tanh',\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "nngmm_exp = NeuralNetworkGMM(\n",
    "    nn=mlp_gmm_exp,\n",
    "    n_components=3,\n",
    "    n_samples=500\n",
    ")\n",
    "\n",
    "print(\"Fitting NNGMM on exponential dataset...\")\n",
    "nngmm_exp.fit(X_train_exp, y_train_exp)\n",
    "\n",
    "y_pred_gmm_exp, y_std_gmm_exp = nngmm_exp.predict(X_test_exp, return_std=True)\n",
    "y_pred_gmm_exp = y_pred_gmm_exp.flatten()\n",
    "\n",
    "y_lower_gmm_exp = y_pred_gmm_exp - 1.96 * y_std_gmm_exp\n",
    "y_upper_gmm_exp = y_pred_gmm_exp + 1.96 * y_std_gmm_exp\n",
    "\n",
    "rmse_gmm_exp = np.sqrt(np.mean((y_test_exp - y_pred_gmm_exp)**2))\n",
    "coverage_gmm_exp = np.mean((y_test_exp >= y_lower_gmm_exp) & (y_test_exp <= y_upper_gmm_exp))\n",
    "width_gmm_exp = np.mean(y_upper_gmm_exp - y_lower_gmm_exp)\n",
    "\n",
    "print(f\"NNGMM: Coverage={coverage_gmm_exp:.3f}, RMSE={rmse_gmm_exp:.4f}, Width={width_gmm_exp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NNBR on exponential dataset\n",
    "mlp_br_exp = MLPRegressor(\n",
    "    hidden_layer_sizes=(20,),\n",
    "    activation='tanh',\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "br_exp = BayesianRidge()\n",
    "nnbr_exp = NeuralNetworkBLR(nn=mlp_br_exp, br=br_exp)\n",
    "\n",
    "# Split for calibration\n",
    "X_train_fit_exp, X_train_val_exp, y_train_fit_exp, y_train_val_exp = train_test_split(\n",
    "    X_train_exp, y_train_exp, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Fitting NNBR on exponential dataset with calibration...\")\n",
    "nnbr_exp.fit(X_train_fit_exp, y_train_fit_exp, val_X=X_train_val_exp, val_y=y_train_val_exp)\n",
    "\n",
    "y_pred_br_exp, y_std_br_exp = nnbr_exp.predict(X_test_exp, return_std=True)\n",
    "if y_pred_br_exp.ndim > 1:\n",
    "    y_pred_br_exp = y_pred_br_exp.flatten()\n",
    "\n",
    "y_lower_br_exp = y_pred_br_exp - 1.96 * y_std_br_exp\n",
    "y_upper_br_exp = y_pred_br_exp + 1.96 * y_std_br_exp\n",
    "\n",
    "rmse_br_exp = np.sqrt(np.mean((y_test_exp - y_pred_br_exp)**2))\n",
    "coverage_br_exp = np.mean((y_test_exp >= y_lower_br_exp) & (y_test_exp <= y_upper_br_exp))\n",
    "width_br_exp = np.mean(y_upper_br_exp - y_lower_br_exp)\n",
    "\n",
    "print(f\"NNBR: Coverage={coverage_br_exp:.3f}, RMSE={rmse_br_exp:.4f}, Width={width_br_exp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison on exponential dataset\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# NNGMM plot\n",
    "ax = axes[0]\n",
    "sort_idx = np.argsort(X_test_exp.flatten())\n",
    "X_sorted = X_test_exp[sort_idx].flatten()\n",
    "\n",
    "ax.scatter(X_train_exp, y_train_exp, alpha=0.6, s=30, label='Training data', color='blue')\n",
    "ax.scatter(X_test_exp, y_test_exp, alpha=0.3, s=20, label='Test data', color='gray')\n",
    "ax.plot(X_sorted, y_pred_gmm_exp[sort_idx], 'r-', linewidth=2, label='NNGMM prediction')\n",
    "ax.fill_between(X_sorted, y_lower_gmm_exp[sort_idx], y_upper_gmm_exp[sort_idx], \n",
    "                 alpha=0.3, color='red', label='95% interval')\n",
    "\n",
    "# Plot true function\n",
    "X_plot = np.linspace(0, 1, 200)\n",
    "y_true_exp = exp_dataset._generate_clean(X_plot)\n",
    "ax.plot(X_plot, y_true_exp, 'k--', linewidth=2, label='True function', zorder=10)\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title(f'NNGMM - Exponential Decay\\nCov={coverage_gmm_exp:.3f}, Width={width_gmm_exp:.4f}, RMSE={rmse_gmm_exp:.4f}')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# NNBR plot\n",
    "ax = axes[1]\n",
    "ax.scatter(X_train_exp, y_train_exp, alpha=0.6, s=30, label='Training data', color='blue')\n",
    "ax.scatter(X_test_exp, y_test_exp, alpha=0.3, s=20, label='Test data', color='gray')\n",
    "ax.plot(X_sorted, y_pred_br_exp[sort_idx], 'g-', linewidth=2, label='NNBR prediction')\n",
    "ax.fill_between(X_sorted, y_lower_br_exp[sort_idx], y_upper_br_exp[sort_idx], \n",
    "                 alpha=0.3, color='green', label='95% interval (calibrated)')\n",
    "ax.plot(X_plot, y_true_exp, 'k--', linewidth=2, label='True function', zorder=10)\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title(f'NNBR - Exponential Decay\\nCov={coverage_br_exp:.3f}, Width={width_br_exp:.4f}, RMSE={rmse_br_exp:.4f}')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Benchmark Results: Large-Scale Comparison\n",
    "\n",
    "We've trained both methods on 56 different configurations (7 datasets × 2 noise models × 4 noise levels). Here are the key findings from the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmark results\n",
    "nnbr_results = pd.read_csv('../../results/nnbr_fits/nnbr_results_summary.csv')\n",
    "nngmm_results = pd.read_csv('../../results/nngmm_fits/nngmm_results_summary.csv')\n",
    "\n",
    "# Convert coverage and RMSE to numeric\n",
    "nnbr_results['Coverage_num'] = nnbr_results['Coverage'].astype(float)\n",
    "nnbr_results['RMSE_num'] = nnbr_results['RMSE'].astype(float)\n",
    "nnbr_results['Width_num'] = nnbr_results['Mean Width'].astype(float)\n",
    "\n",
    "nngmm_results['Coverage_num'] = nngmm_results['Coverage'].astype(float)\n",
    "nngmm_results['RMSE_num'] = nngmm_results['RMSE'].astype(float)\n",
    "nngmm_results['Width_num'] = nngmm_results['Mean Width'].astype(float)\n",
    "\n",
    "print(\"NNBR Benchmark Summary (56 experiments):\")\n",
    "print(f\"  Average Coverage: {nnbr_results['Coverage_num'].mean():.3f}\")\n",
    "print(f\"  Average RMSE: {nnbr_results['RMSE_num'].mean():.4f}\")\n",
    "print(f\"  Average Width: {nnbr_results['Width_num'].mean():.4f}\")\n",
    "print(f\"  Coverage std: {nnbr_results['Coverage_num'].std():.3f}\")\n",
    "\n",
    "print(\"\\nNNGMM Benchmark Summary (56 experiments):\")\n",
    "print(f\"  Average Coverage: {nngmm_results['Coverage_num'].mean():.3f}\")\n",
    "print(f\"  Average RMSE: {nngmm_results['RMSE_num'].mean():.4f}\")\n",
    "print(f\"  Average Width: {nngmm_results['Width_num'].mean():.4f}\")\n",
    "print(f\"  Coverage std: {nngmm_results['Coverage_num'].std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize benchmark comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Coverage comparison\n",
    "ax = axes[0]\n",
    "ax.hist(nnbr_results['Coverage_num'], bins=20, alpha=0.6, label='NNBR', color='green')\n",
    "ax.hist(nngmm_results['Coverage_num'], bins=20, alpha=0.6, label='NNGMM', color='red')\n",
    "ax.axvline(0.95, color='black', linestyle='--', linewidth=2, label='Target (0.95)')\n",
    "ax.set_xlabel('Coverage')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Coverage Distribution\\n(56 experiments each)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE comparison\n",
    "ax = axes[1]\n",
    "ax.hist(nnbr_results['RMSE_num'], bins=20, alpha=0.6, label='NNBR', color='green')\n",
    "ax.hist(nngmm_results['RMSE_num'], bins=20, alpha=0.6, label='NNGMM', color='red')\n",
    "ax.set_xlabel('RMSE')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('RMSE Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Width comparison\n",
    "ax = axes[2]\n",
    "ax.hist(nnbr_results['Width_num'], bins=20, alpha=0.6, label='NNBR', color='green')\n",
    "ax.hist(nngmm_results['Width_num'], bins=20, alpha=0.6, label='NNGMM', color='red')\n",
    "ax.set_xlabel('Mean Interval Width')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Interval Width Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count experiments with good coverage (0.90 - 0.98)\n",
    "good_coverage_nnbr = ((nnbr_results['Coverage_num'] >= 0.90) & \n",
    "                      (nnbr_results['Coverage_num'] <= 0.98)).sum()\n",
    "good_coverage_nngmm = ((nngmm_results['Coverage_num'] >= 0.90) & \n",
    "                       (nngmm_results['Coverage_num'] <= 0.98)).sum()\n",
    "\n",
    "print(f\"Experiments with good coverage (0.90-0.98):\")\n",
    "print(f\"  NNBR: {good_coverage_nnbr}/56 ({100*good_coverage_nnbr/56:.1f}%)\")\n",
    "print(f\"  NNGMM: {good_coverage_nngmm}/56 ({100*good_coverage_nngmm/56:.1f}%)\")\n",
    "\n",
    "# Count experiments with negative R² (indicates poor fit)\n",
    "nnbr_results['R2_num'] = nnbr_results['R²'].astype(float)\n",
    "nngmm_results['R2_num'] = nngmm_results['R²'].astype(float)\n",
    "\n",
    "negative_r2_nnbr = (nnbr_results['R2_num'] < 0).sum()\n",
    "negative_r2_nngmm = (nngmm_results['R2_num'] < 0).sum()\n",
    "\n",
    "print(f\"\\nExperiments with negative R² (poor fits):\")\n",
    "print(f\"  NNBR: {negative_r2_nnbr}/56 ({100*negative_r2_nnbr/56:.1f}%)\")\n",
    "print(f\"  NNGMM: {negative_r2_nngmm}/56 ({100*negative_r2_nngmm/56:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings and Practical Recommendations\n",
    "\n",
    "### NNBR Strengths:\n",
    "\n",
    "1. **Better Calibration**: Post-hoc calibration using validation data significantly improves coverage\n",
    "   - NNBR achieves closer to target 95% coverage on average\n",
    "   - More consistent coverage across different datasets\n",
    "\n",
    "2. **More Stable**: Bayesian linear regression is more robust than GMM fitting\n",
    "   - Fewer catastrophic failures (negative R²)\n",
    "   - More reliable across diverse problem types\n",
    "\n",
    "3. **Computational Efficiency**: Faster and simpler than GMM approach\n",
    "\n",
    "### NNGMM Challenges:\n",
    "\n",
    "1. **Unstable GMM Fitting**: \n",
    "   - GMM can diverge or produce poor fits, especially on complex datasets\n",
    "   - Many experiments show negative R² values\n",
    "   - Coverage is often far from target\n",
    "\n",
    "2. **Lack of Calibration**: No built-in mechanism to adjust uncertainties based on validation performance\n",
    "\n",
    "3. **Sensitivity to Hyperparameters**: Number of GMM components affects performance\n",
    "\n",
    "### Practical Recommendations:\n",
    "\n",
    "1. **Default Choice**: Use **NNBR** for most applications\n",
    "   - More reliable and stable\n",
    "   - Better calibrated uncertainties\n",
    "   - Easier to tune\n",
    "\n",
    "2. **When to Consider NNGMM**:\n",
    "   - If you suspect truly multimodal prediction distributions\n",
    "   - For simple, well-behaved problems where GMM is stable\n",
    "   - When you can afford to validate GMM quality\n",
    "\n",
    "3. **Best Practices for NNBR**:\n",
    "   - Always use validation split for calibration (20% of training data)\n",
    "   - Use 1.96 × std for 95% intervals (Gaussian assumption)\n",
    "   - Verify calibration quality on held-out data\n",
    "\n",
    "4. **Neural Network Architecture**:\n",
    "   - Both methods benefit from similar NN architectures\n",
    "   - Start with 20 hidden units, tanh activation\n",
    "   - Use LBFGS solver for small-medium datasets\n",
    "\n",
    "### Calibration Quality:\n",
    "\n",
    "The key advantage of NNBR is its calibration procedure:\n",
    "- Learns a scaling factor for uncertainties from validation data\n",
    "- Corrects for under/over-confident predictions\n",
    "- Results in coverage closer to nominal level (95%)\n",
    "\n",
    "NNGMM lacks this mechanism, leading to:\n",
    "- Highly variable coverage (often too low)\n",
    "- No automatic adjustment for miscalibration\n",
    "- Requires manual tuning of GMM components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **NNGMM**: Neural network features + GMM for uncertainty\n",
    "   - Flexible but unstable\n",
    "   - Prone to poor fits on some datasets\n",
    "   - No calibration mechanism\n",
    "\n",
    "2. **NNBR**: Neural network features + Bayesian regression\n",
    "   - More stable and reliable\n",
    "   - Post-hoc calibration improves coverage\n",
    "   - Better performance across diverse datasets\n",
    "\n",
    "3. **Benchmark Comparison**: Across 56 experiments\n",
    "   - NNBR shows superior calibration and stability\n",
    "   - NNGMM struggles on many datasets\n",
    "\n",
    "4. **Recommendation**: Use NNBR as the default neural network UQ method\n",
    "   - More predictable behavior\n",
    "   - Better calibrated uncertainties\n",
    "   - Easier to deploy in practice\n",
    "\n",
    "### References\n",
    "\n",
    "- **pycse library**: https://github.com/jkitchin/pycse\n",
    "- **NeuralNetworkGMM**: `pycse.sklearn.nngmm.NeuralNetworkGMM`\n",
    "- **NeuralNetworkBLR**: `pycse.sklearn.nnbr.NeuralNetworkBLR`\n",
    "- Benchmark results: `results/nnbr_fits/` and `results/nngmm_fits/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
