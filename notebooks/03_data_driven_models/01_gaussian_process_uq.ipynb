{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression for Uncertainty Quantification\n",
    "\n",
    "This notebook provides a comprehensive introduction to **Gaussian Process (GP) regression** for uncertainty quantification. Gaussian Processes are a powerful non-parametric Bayesian approach that naturally provides predictive uncertainty estimates.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **GP Fundamentals**: Understanding GPs as distributions over functions\n",
    "2. **Kernel Functions**: RBF and Matern kernels for modeling different smoothness assumptions\n",
    "3. **Hyperparameter Optimization**: Automatic tuning via maximum likelihood\n",
    "4. **Uncertainty Quantification**: Mean predictions and confidence intervals\n",
    "5. **Interpolation vs Extrapolation**: How GP uncertainty changes with distance from training data\n",
    "6. **Coverage Analysis**: Validating prediction intervals\n",
    "7. **Noise Models**: Handling homoskedastic vs heteroskedastic noise\n",
    "\n",
    "## References\n",
    "\n",
    "- Rasmussen & Williams (2006). *Gaussian Processes for Machine Learning*\n",
    "- scikit-learn GP documentation: https://scikit-learn.org/stable/modules/gaussian_process.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "# Gaussian Process from sklearn\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import (\n",
    "    RBF, Matern, WhiteKernel, ConstantKernel as C\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Dataset utilities\n",
    "from src.datasets.nonlinear import GaussianDataset, ExponentialDecayDataset\n",
    "from src.datasets.generators import generate_noise\n",
    "\n",
    "# Visualization\n",
    "from src.visualization import setup_plot_style\n",
    "\n",
    "# Metrics\n",
    "from src.metrics import picp, mean_interval_width\n",
    "\n",
    "# Set up plotting style\n",
    "setup_plot_style()\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gaussian Processes: The Basics\n",
    "\n",
    "### What is a Gaussian Process?\n",
    "\n",
    "A **Gaussian Process** is a collection of random variables, any finite number of which have a joint Gaussian distribution. In the context of regression:\n",
    "\n",
    "- **Prior**: We assume that the unknown function $f(x)$ follows a GP prior: $f(x) \\sim \\mathcal{GP}(m(x), k(x, x'))$\n",
    "  - $m(x)$ is the mean function (often set to 0)\n",
    "  - $k(x, x')$ is the covariance (kernel) function that encodes our assumptions about smoothness\n",
    "\n",
    "- **Posterior**: After observing training data $(X_{\\text{train}}, y_{\\text{train}})$, we get a posterior GP:\n",
    "  $$f(x_*) | X_{\\text{train}}, y_{\\text{train}}, x_* \\sim \\mathcal{N}(\\mu(x_*), \\sigma^2(x_*))$$\n",
    "  \n",
    "  where:\n",
    "  - $\\mu(x_*)$ is the predictive mean\n",
    "  - $\\sigma^2(x_*)$ is the predictive variance\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "1. **Non-parametric**: GPs don't assume a fixed functional form\n",
    "2. **Bayesian**: Naturally provides uncertainty quantification\n",
    "3. **Kernel-based**: The kernel function encodes prior knowledge about function smoothness\n",
    "4. **Exact inference**: Closed-form posterior (for Gaussian likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GP prior samples\n",
    "def plot_gp_prior_samples(kernel, n_samples=5, n_points=200):\n",
    "    \"\"\"\n",
    "    Visualize samples from a GP prior.\n",
    "    \n",
    "    This shows what kind of functions the GP believes are plausible\n",
    "    BEFORE seeing any data.\n",
    "    \"\"\"\n",
    "    X_plot = np.linspace(0, 1, n_points).reshape(-1, 1)\n",
    "    \n",
    "    # Create GP with the kernel\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, random_state=42)\n",
    "    \n",
    "    # Sample from prior\n",
    "    y_samples = gp.sample_y(X_plot, n_samples=n_samples)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    for i in range(n_samples):\n",
    "        ax.plot(X_plot, y_samples[:, i], alpha=0.7, linewidth=1.5, label=f'Sample {i+1}')\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('f(x)')\n",
    "    ax.set_title(f'GP Prior Samples\\nKernel: {kernel}')\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Try different kernels\n",
    "print(\"RBF Kernel (Squared Exponential):\")\n",
    "print(\"Infinitely differentiable, very smooth functions\")\n",
    "kernel_rbf = C(1.0) * RBF(length_scale=0.1)\n",
    "plot_gp_prior_samples(kernel_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMatern Kernel (nu=1.5):\")\n",
    "print(\"Once differentiable, less smooth than RBF\")\n",
    "kernel_matern = C(1.0) * Matern(length_scale=0.1, nu=1.5)\n",
    "plot_gp_prior_samples(kernel_matern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kernel Selection and Hyperparameters\n",
    "\n",
    "### Common Kernels\n",
    "\n",
    "**RBF (Radial Basis Function / Squared Exponential)**:\n",
    "$$k(x, x') = \\sigma^2 \\exp\\left(-\\frac{\\|x - x'\\|^2}{2\\ell^2}\\right)$$\n",
    "- $\\sigma^2$: output variance (amplitude)\n",
    "- $\\ell$: length scale (controls smoothness)\n",
    "- Produces infinitely smooth functions\n",
    "\n",
    "**Matern Kernel**:\n",
    "$$k(x, x') = \\sigma^2 \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left(\\frac{\\sqrt{2\\nu}\\|x - x'\\|}{\\ell}\\right)^\\nu K_\\nu\\left(\\frac{\\sqrt{2\\nu}\\|x - x'\\|}{\\ell}\\right)$$\n",
    "- $\\nu$ controls smoothness ($\\nu = 1/2, 3/2, 5/2$ common)\n",
    "- Less smooth than RBF, often more realistic\n",
    "\n",
    "**White Kernel (Noise)**:\n",
    "$$k(x, x') = \\sigma_n^2 \\delta_{x,x'}$$\n",
    "- Models observation noise\n",
    "- Only non-zero when $x = x'$\n",
    "\n",
    "### Hyperparameter Optimization\n",
    "\n",
    "Hyperparameters (like length scale and variance) are automatically optimized by maximizing the **marginal likelihood**:\n",
    "$$\\log p(y | X, \\theta) = -\\frac{1}{2}y^T K^{-1} y - \\frac{1}{2}\\log|K| - \\frac{n}{2}\\log(2\\pi)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate hyperparameter optimization\n",
    "def demonstrate_hyperparameter_optimization():\n",
    "    \"\"\"Show how GP optimizes kernel hyperparameters.\"\"\"\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    np.random.seed(42)\n",
    "    X_train = np.random.uniform(0.2, 0.8, 15).reshape(-1, 1)\n",
    "    y_train = np.sin(10 * X_train).flatten() + 0.1 * np.random.randn(15)\n",
    "    \n",
    "    X_test = np.linspace(0, 1, 200).reshape(-1, 1)\n",
    "    \n",
    "    # Define kernel with bounds for hyperparameters\n",
    "    kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "    \n",
    "    print(f\"Initial kernel: {kernel}\")\n",
    "    \n",
    "    # Fit GP\n",
    "    gp = GaussianProcessRegressor(\n",
    "        kernel=kernel,\n",
    "        n_restarts_optimizer=10,  # Try 10 random starting points\n",
    "        random_state=42\n",
    "    )\n",
    "    gp.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\nOptimized kernel: {gp.kernel_}\")\n",
    "    print(f\"Log-marginal-likelihood: {gp.log_marginal_likelihood(gp.kernel_.theta):.3f}\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred, y_std = gp.predict(X_test, return_std=True)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    # Training data\n",
    "    ax.scatter(X_train, y_train, c='red', s=80, alpha=0.8, edgecolors='black', \n",
    "               linewidth=1.5, label='Training data', zorder=10)\n",
    "    \n",
    "    # GP mean and uncertainty\n",
    "    ax.plot(X_test, y_pred, 'b-', linewidth=2, label='GP mean', zorder=5)\n",
    "    ax.fill_between(X_test.flatten(), \n",
    "                     y_pred - 1.96*y_std, \n",
    "                     y_pred + 1.96*y_std,\n",
    "                     alpha=0.3, color='blue', label='95% confidence', zorder=1)\n",
    "    ax.fill_between(X_test.flatten(), \n",
    "                     y_pred - y_std, \n",
    "                     y_pred + y_std,\n",
    "                     alpha=0.5, color='blue', label='68% confidence', zorder=2)\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title('GP with Optimized Hyperparameters')\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "demonstrate_hyperparameter_optimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GP Regression on Nonlinear Data\n",
    "\n",
    "Let's apply GP regression to a **Gaussian (bell curve)** function with noise. This is a common nonlinear pattern in scientific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Gaussian dataset\n",
    "dataset = GaussianDataset(\n",
    "    n_samples=100,\n",
    "    noise_model='homoskedastic',\n",
    "    noise_level=0.05,  # 5% noise\n",
    "    seed=42,\n",
    "    a=1.0,\n",
    "    mu=0.5,\n",
    "    sigma=0.15\n",
    ")\n",
    "\n",
    "# Generate data\n",
    "data = dataset.generate()\n",
    "\n",
    "print(f\"Dataset: {dataset.get_function_form()}\")\n",
    "print(f\"Training samples: {len(data.X_train)}\")\n",
    "print(f\"Test samples: {len(data.X_test)}\")\n",
    "print(f\"Gap samples: {len(data.X_gap)}\")\n",
    "print(f\"Noise model: {dataset.noise_model}\")\n",
    "print(f\"Noise level: {dataset.noise_level}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot regions\n",
    "ax.scatter(data.X_train, data.y_train, alpha=0.7, s=50, \n",
    "           label='Training data', color='blue', edgecolors='black', linewidth=0.5)\n",
    "ax.scatter(data.X_gap, data.y_gap, alpha=0.7, s=50, \n",
    "           label='Gap (interpolation test)', color='orange', marker='s')\n",
    "ax.scatter(data.X_extrap_low, data.y_extrap_low, alpha=0.7, s=50, \n",
    "           label='Low extrapolation', color='red', marker='^')\n",
    "ax.scatter(data.X_extrap_high, data.y_extrap_high, alpha=0.7, s=50, \n",
    "           label='High extrapolation', color='green', marker='v')\n",
    "\n",
    "# True function\n",
    "X_true = np.linspace(0, 1, 300).reshape(-1, 1)\n",
    "y_true = dataset._generate_clean(X_true.flatten())\n",
    "ax.plot(X_true, y_true, 'k--', linewidth=2, label='True function', zorder=10)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Gaussian Function Dataset with Homoskedastic Noise')\n",
    "ax.legend(loc='best', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fitting a Gaussian Process Model\n",
    "\n",
    "We'll fit a GP with an **RBF kernel** and **White kernel** for noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_train = data.X_train.reshape(-1, 1)\n",
    "y_train = data.y_train\n",
    "X_test = data.X_test.reshape(-1, 1)\n",
    "y_test = data.y_test\n",
    "\n",
    "# Combine test and gap for full evaluation\n",
    "X_gap = data.X_gap.reshape(-1, 1) if data.X_gap is not None else np.array([]).reshape(-1, 1)\n",
    "y_gap = data.y_gap if data.y_gap is not None else np.array([])\n",
    "\n",
    "if len(X_gap) > 0:\n",
    "    X_eval = np.vstack([X_test, X_gap])\n",
    "    y_eval = np.hstack([y_test, y_gap])\n",
    "else:\n",
    "    X_eval = X_test\n",
    "    y_eval = y_test\n",
    "\n",
    "# Define kernel: ConstantKernel * RBF + WhiteKernel\n",
    "kernel = (\n",
    "    C(1.0, (1e-3, 1e3)) * RBF(length_scale=0.1, length_scale_bounds=(1e-3, 1.0)) +\n",
    "    WhiteKernel(noise_level=0.01, noise_level_bounds=(1e-5, 1e1))\n",
    ")\n",
    "\n",
    "print(\"Initial kernel:\")\n",
    "print(kernel)\n",
    "\n",
    "# Create and fit GP\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    n_restarts_optimizer=10,\n",
    "    alpha=1e-10,\n",
    "    normalize_y=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nFitting Gaussian Process...\")\n",
    "gp.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nOptimized kernel:\")\n",
    "print(gp.kernel_)\n",
    "print(f\"\\nLog-marginal-likelihood: {gp.log_marginal_likelihood(gp.kernel_.theta):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred, y_std = gp.predict(X_eval, return_std=True)\n",
    "\n",
    "# Compute 95% prediction intervals\n",
    "z_score = 1.96  # 95% confidence\n",
    "y_lower = y_pred - z_score * y_std\n",
    "y_upper = y_pred + z_score * y_std\n",
    "\n",
    "# Compute metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_eval, y_pred))\n",
    "coverage = np.mean((y_eval >= y_lower) & (y_eval <= y_upper))\n",
    "mean_width = np.mean(y_upper - y_lower)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GP PERFORMANCE METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"RMSE:                {rmse:.4f}\")\n",
    "print(f\"Coverage (95% PI):   {coverage:.3f} (target: 0.95)\")\n",
    "print(f\"Mean Interval Width: {mean_width:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Uncertainty Visualization\n",
    "\n",
    "Let's visualize the GP predictions with uncertainty bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dense grid for smooth plotting\n",
    "X_plot = np.linspace(0, 1, 300).reshape(-1, 1)\n",
    "y_pred_plot, y_std_plot = gp.predict(X_plot, return_std=True)\n",
    "y_lower_plot = y_pred_plot - 1.96 * y_std_plot\n",
    "y_upper_plot = y_pred_plot + 1.96 * y_std_plot\n",
    "\n",
    "# True function\n",
    "y_true_plot = dataset._generate_clean(X_plot.flatten())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Uncertainty bands (plot first so they're in background)\n",
    "ax.fill_between(X_plot.flatten(), y_lower_plot, y_upper_plot,\n",
    "                 alpha=0.2, color='blue', label='95% Prediction Interval')\n",
    "ax.fill_between(X_plot.flatten(), \n",
    "                 y_pred_plot - y_std_plot, \n",
    "                 y_pred_plot + y_std_plot,\n",
    "                 alpha=0.4, color='blue', label='68% Prediction Interval')\n",
    "\n",
    "# Predictions and true function\n",
    "ax.plot(X_plot, y_pred_plot, 'b-', linewidth=2.5, label='GP Mean Prediction', zorder=5)\n",
    "ax.plot(X_plot, y_true_plot, 'k--', linewidth=2, label='True Function', zorder=6)\n",
    "\n",
    "# Training data\n",
    "ax.scatter(X_train, y_train, c='red', s=70, alpha=0.8, \n",
    "           edgecolors='black', linewidth=1, label='Training Data', zorder=10)\n",
    "\n",
    "# Highlight regions\n",
    "ax.axvspan(0, 0.125, alpha=0.05, color='red', label='Low Extrapolation')\n",
    "ax.axvspan(0.875, 1.0, alpha=0.05, color='green', label='High Extrapolation')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Gaussian Process Regression with Uncertainty Quantification', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"1. Uncertainty (shaded region) is SMALL near training data\")\n",
    "print(\"2. Uncertainty INCREASES in extrapolation regions (x < 0.125 and x > 0.875)\")\n",
    "print(\"3. GP mean closely follows the true function in interpolation region\")\n",
    "print(\"4. In extrapolation regions, GP reverts to prior mean with high uncertainty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing Uncertainty Growth in Extrapolation\n",
    "\n",
    "One of the key features of GPs is that uncertainty **increases** as we move away from training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot standard deviation vs x\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "# Top plot: Predictions with uncertainty\n",
    "axes[0].fill_between(X_plot.flatten(), y_lower_plot, y_upper_plot,\n",
    "                      alpha=0.3, color='blue')\n",
    "axes[0].plot(X_plot, y_pred_plot, 'b-', linewidth=2, label='GP Mean')\n",
    "axes[0].plot(X_plot, y_true_plot, 'k--', linewidth=2, label='True Function')\n",
    "axes[0].scatter(X_train, y_train, c='red', s=50, alpha=0.8, \n",
    "                edgecolors='black', linewidth=1, label='Training Data', zorder=10)\n",
    "axes[0].set_ylabel('y', fontsize=12)\n",
    "axes[0].set_title('GP Predictions and 95% Confidence Bands', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(loc='best')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom plot: Standard deviation\n",
    "axes[1].plot(X_plot, y_std_plot, 'r-', linewidth=2.5, label='Predictive Std Dev')\n",
    "axes[1].axvspan(0, 0.125, alpha=0.1, color='red', label='Low Extrap')\n",
    "axes[1].axvspan(0.875, 1.0, alpha=0.1, color='green', label='High Extrap')\n",
    "axes[1].set_xlabel('x', fontsize=12)\n",
    "axes[1].set_ylabel('Standard Deviation', fontsize=12)\n",
    "axes[1].set_title('Uncertainty (Standard Deviation) vs Position', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(loc='best')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Uncertainty is LOWEST near training data points\")\n",
    "print(\"- Uncertainty INCREASES dramatically in extrapolation regions\")\n",
    "print(\"- This is a key advantage of GPs: honest uncertainty quantification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Effect of Different Kernels\n",
    "\n",
    "Let's compare RBF vs Matern kernels on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different kernels\n",
    "kernels = {\n",
    "    'RBF': C(1.0, (1e-3, 1e3)) * RBF(0.1, (1e-3, 1.0)) + WhiteKernel(0.01, (1e-5, 1e1)),\n",
    "    'Matern (nu=1.5)': C(1.0, (1e-3, 1e3)) * Matern(0.1, (1e-3, 1.0), nu=1.5) + WhiteKernel(0.01, (1e-5, 1e1)),\n",
    "    'Matern (nu=2.5)': C(1.0, (1e-3, 1e3)) * Matern(0.1, (1e-3, 1.0), nu=2.5) + WhiteKernel(0.01, (1e-5, 1e1)),\n",
    "}\n",
    "\n",
    "# Fit GPs with different kernels\n",
    "gp_results = {}\n",
    "\n",
    "for name, kernel in kernels.items():\n",
    "    print(f\"\\nFitting GP with {name} kernel...\")\n",
    "    gp_temp = GaussianProcessRegressor(\n",
    "        kernel=kernel,\n",
    "        n_restarts_optimizer=10,\n",
    "        alpha=1e-10,\n",
    "        normalize_y=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    gp_temp.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_temp, y_std_temp = gp_temp.predict(X_plot, return_std=True)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_eval, y_std_eval = gp_temp.predict(X_eval, return_std=True)\n",
    "    y_lower_eval = y_pred_eval - 1.96 * y_std_eval\n",
    "    y_upper_eval = y_pred_eval + 1.96 * y_std_eval\n",
    "    \n",
    "    rmse_temp = np.sqrt(mean_squared_error(y_eval, y_pred_eval))\n",
    "    coverage_temp = np.mean((y_eval >= y_lower_eval) & (y_eval <= y_upper_eval))\n",
    "    width_temp = np.mean(y_upper_eval - y_lower_eval)\n",
    "    \n",
    "    gp_results[name] = {\n",
    "        'gp': gp_temp,\n",
    "        'y_pred': y_pred_temp,\n",
    "        'y_std': y_std_temp,\n",
    "        'rmse': rmse_temp,\n",
    "        'coverage': coverage_temp,\n",
    "        'width': width_temp\n",
    "    }\n",
    "    \n",
    "    print(f\"  Optimized kernel: {gp_temp.kernel_}\")\n",
    "    print(f\"  RMSE: {rmse_temp:.4f}, Coverage: {coverage_temp:.3f}, Width: {width_temp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, (name, result) in zip(axes, gp_results.items()):\n",
    "    y_pred_temp = result['y_pred']\n",
    "    y_std_temp = result['y_std']\n",
    "    \n",
    "    # Uncertainty bands\n",
    "    ax.fill_between(X_plot.flatten(), \n",
    "                     y_pred_temp - 1.96*y_std_temp, \n",
    "                     y_pred_temp + 1.96*y_std_temp,\n",
    "                     alpha=0.3, color='blue', label='95% CI')\n",
    "    \n",
    "    # Predictions\n",
    "    ax.plot(X_plot, y_pred_temp, 'b-', linewidth=2, label='GP Mean')\n",
    "    ax.plot(X_plot, y_true_plot, 'k--', linewidth=1.5, label='True Function')\n",
    "    \n",
    "    # Training data\n",
    "    ax.scatter(X_train, y_train, c='red', s=40, alpha=0.8, \n",
    "               edgecolors='black', linewidth=0.5, label='Training', zorder=10)\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'{name}\\nCoverage: {result[\"coverage\"]:.3f}, RMSE: {result[\"rmse\"]:.4f}')\n",
    "    ax.legend(loc='best', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKernel Comparison:\")\n",
    "print(\"- RBF: Very smooth, infinite differentiability\")\n",
    "print(\"- Matern(1.5): Once differentiable, allows rougher functions\")\n",
    "print(\"- Matern(2.5): Twice differentiable, smoother than Matern(1.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Coverage Analysis\n",
    "\n",
    "Let's analyze prediction interval coverage across different regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regions\n",
    "X_eval_flat = X_eval.flatten()\n",
    "\n",
    "regions = {\n",
    "    'Low Extrap': (X_eval_flat < 0.125),\n",
    "    'Interpolation': (X_eval_flat >= 0.2) & (X_eval_flat <= 0.8),\n",
    "    'Gap': (X_eval_flat >= 0.375) & (X_eval_flat <= 0.625),\n",
    "    'High Extrap': (X_eval_flat > 0.875),\n",
    "}\n",
    "\n",
    "# Compute coverage by region for each kernel\n",
    "coverage_by_region = {}\n",
    "\n",
    "for name, result in gp_results.items():\n",
    "    gp_temp = result['gp']\n",
    "    y_pred_eval, y_std_eval = gp_temp.predict(X_eval, return_std=True)\n",
    "    y_lower_eval = y_pred_eval - 1.96 * y_std_eval\n",
    "    y_upper_eval = y_pred_eval + 1.96 * y_std_eval\n",
    "    \n",
    "    coverage_by_region[name] = {}\n",
    "    for region_name, mask in regions.items():\n",
    "        if np.sum(mask) > 0:\n",
    "            y_region = y_eval[mask]\n",
    "            y_lower_region = y_lower_eval[mask]\n",
    "            y_upper_region = y_upper_eval[mask]\n",
    "            coverage_region = np.mean((y_region >= y_lower_region) & (y_region <= y_upper_region))\n",
    "            coverage_by_region[name][region_name] = coverage_region\n",
    "        else:\n",
    "            coverage_by_region[name][region_name] = np.nan\n",
    "\n",
    "# Create DataFrame\n",
    "coverage_df = pd.DataFrame(coverage_by_region).T\n",
    "print(\"\\nCoverage by Region (95% Prediction Intervals):\")\n",
    "print(\"=\"*60)\n",
    "print(coverage_df.to_string())\n",
    "print(\"=\"*60)\n",
    "print(\"Target coverage: 0.95\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coverage by region\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x_pos = np.arange(len(regions))\n",
    "width = 0.25\n",
    "\n",
    "for i, (name, coverages) in enumerate(coverage_by_region.items()):\n",
    "    values = [coverages.get(region, 0) for region in regions.keys()]\n",
    "    ax.bar(x_pos + i*width, values, width, label=name, alpha=0.8)\n",
    "\n",
    "ax.axhline(y=0.95, color='red', linestyle='--', linewidth=2, label='Target (0.95)')\n",
    "ax.set_xlabel('Region', fontsize=12)\n",
    "ax.set_ylabel('Coverage', fontsize=12)\n",
    "ax.set_title('Prediction Interval Coverage by Region and Kernel', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos + width)\n",
    "ax.set_xticklabels(regions.keys())\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0, 1.05])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Homoskedastic vs Heteroskedastic Noise\n",
    "\n",
    "GPs can handle both constant noise (homoskedastic) and input-dependent noise (heteroskedastic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets with different noise models\n",
    "dataset_homo = GaussianDataset(\n",
    "    n_samples=100,\n",
    "    noise_model='homoskedastic',\n",
    "    noise_level=0.05,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "dataset_hetero = GaussianDataset(\n",
    "    n_samples=100,\n",
    "    noise_model='heteroskedastic',\n",
    "    noise_level=0.10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "data_homo = dataset_homo.generate()\n",
    "data_hetero = dataset_hetero.generate()\n",
    "\n",
    "# Fit GPs\n",
    "datasets_noise = {\n",
    "    'Homoskedastic': data_homo,\n",
    "    'Heteroskedastic': data_hetero\n",
    "}\n",
    "\n",
    "gp_noise_results = {}\n",
    "\n",
    "for noise_type, data_temp in datasets_noise.items():\n",
    "    X_train_temp = data_temp.X_train.reshape(-1, 1)\n",
    "    y_train_temp = data_temp.y_train\n",
    "    X_test_temp = data_temp.X_test.reshape(-1, 1)\n",
    "    y_test_temp = data_temp.y_test\n",
    "    \n",
    "    # Define kernel\n",
    "    kernel_temp = (\n",
    "        C(1.0, (1e-3, 1e3)) * RBF(0.1, (1e-3, 1.0)) +\n",
    "        WhiteKernel(0.01, (1e-5, 1e1))\n",
    "    )\n",
    "    \n",
    "    # Fit GP\n",
    "    gp_temp = GaussianProcessRegressor(\n",
    "        kernel=kernel_temp,\n",
    "        n_restarts_optimizer=10,\n",
    "        alpha=1e-10,\n",
    "        normalize_y=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    gp_temp.fit(X_train_temp, y_train_temp)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_temp, y_std_temp = gp_temp.predict(X_plot, return_std=True)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_test, y_std_test = gp_temp.predict(X_test_temp, return_std=True)\n",
    "    y_lower_test = y_pred_test - 1.96 * y_std_test\n",
    "    y_upper_test = y_pred_test + 1.96 * y_std_test\n",
    "    \n",
    "    coverage_temp = np.mean((y_test_temp >= y_lower_test) & (y_test_temp <= y_upper_test))\n",
    "    \n",
    "    gp_noise_results[noise_type] = {\n",
    "        'data': data_temp,\n",
    "        'gp': gp_temp,\n",
    "        'y_pred': y_pred_temp,\n",
    "        'y_std': y_std_temp,\n",
    "        'coverage': coverage_temp\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{noise_type} Noise:\")\n",
    "    print(f\"  Optimized kernel: {gp_temp.kernel_}\")\n",
    "    print(f\"  Coverage: {coverage_temp:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for ax, (noise_type, result) in zip(axes, gp_noise_results.items()):\n",
    "    data_temp = result['data']\n",
    "    y_pred_temp = result['y_pred']\n",
    "    y_std_temp = result['y_std']\n",
    "    \n",
    "    # Uncertainty bands\n",
    "    ax.fill_between(X_plot.flatten(), \n",
    "                     y_pred_temp - 1.96*y_std_temp, \n",
    "                     y_pred_temp + 1.96*y_std_temp,\n",
    "                     alpha=0.3, color='blue', label='95% CI')\n",
    "    \n",
    "    # Predictions\n",
    "    ax.plot(X_plot, y_pred_temp, 'b-', linewidth=2.5, label='GP Mean')\n",
    "    ax.plot(X_plot, y_true_plot, 'k--', linewidth=2, label='True Function')\n",
    "    \n",
    "    # Training data\n",
    "    ax.scatter(data_temp.X_train, data_temp.y_train, c='red', s=50, alpha=0.7, \n",
    "               edgecolors='black', linewidth=0.5, label='Training', zorder=10)\n",
    "    \n",
    "    ax.set_xlabel('x', fontsize=12)\n",
    "    ax.set_ylabel('y', fontsize=12)\n",
    "    ax.set_title(f'{noise_type} Noise\\nCoverage: {result[\"coverage\"]:.3f}', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Homoskedastic: Constant noise level, uniform uncertainty bands\")\n",
    "print(\"- Heteroskedastic: Input-dependent noise, but GP assumes constant noise\")\n",
    "print(\"- GP with WhiteKernel models homoskedastic noise by default\")\n",
    "print(\"- For heteroskedastic noise, consider using input-dependent noise models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Calibration Quality: Expected vs Observed Coverage\n",
    "\n",
    "A well-calibrated GP should have **observed coverage ≈ nominal coverage** across different confidence levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute coverage for different confidence levels\n",
    "confidence_levels = np.linspace(0.5, 0.99, 20)\n",
    "observed_coverages = []\n",
    "\n",
    "for conf_level in confidence_levels:\n",
    "    # Compute z-score for this confidence level\n",
    "    from scipy.stats import norm\n",
    "    z = norm.ppf((1 + conf_level) / 2)\n",
    "    \n",
    "    # Compute intervals\n",
    "    y_pred_calib, y_std_calib = gp.predict(X_eval, return_std=True)\n",
    "    y_lower_calib = y_pred_calib - z * y_std_calib\n",
    "    y_upper_calib = y_pred_calib + z * y_std_calib\n",
    "    \n",
    "    # Compute coverage\n",
    "    coverage_calib = np.mean((y_eval >= y_lower_calib) & (y_eval <= y_upper_calib))\n",
    "    observed_coverages.append(coverage_calib)\n",
    "\n",
    "observed_coverages = np.array(observed_coverages)\n",
    "\n",
    "# Plot calibration curve\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Perfect calibration line\n",
    "ax.plot([0.5, 1.0], [0.5, 1.0], 'k--', linewidth=2, label='Perfect Calibration', zorder=1)\n",
    "\n",
    "# Observed calibration\n",
    "ax.plot(confidence_levels, observed_coverages, 'bo-', linewidth=2.5, \n",
    "        markersize=8, label='GP Calibration', zorder=5)\n",
    "\n",
    "# Shaded region for acceptable calibration\n",
    "ax.fill_between(confidence_levels, confidence_levels - 0.05, confidence_levels + 0.05,\n",
    "                 alpha=0.2, color='green', label='±5% tolerance')\n",
    "\n",
    "ax.set_xlabel('Nominal Coverage (Confidence Level)', fontsize=12)\n",
    "ax.set_ylabel('Observed Coverage', fontsize=12)\n",
    "ax.set_title('GP Calibration Curve: Expected vs Observed Coverage', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim([0.48, 1.0])\n",
    "ax.set_ylim([0.48, 1.0])\n",
    "ax.set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute calibration error\n",
    "calibration_error = np.mean(np.abs(observed_coverages - confidence_levels))\n",
    "print(f\"\\nMean Absolute Calibration Error: {calibration_error:.4f}\")\n",
    "print(f\"Interpretation: Average deviation from perfect calibration\")\n",
    "\n",
    "if calibration_error < 0.05:\n",
    "    print(\"Result: EXCELLENT calibration (error < 5%)\")\n",
    "elif calibration_error < 0.10:\n",
    "    print(\"Result: GOOD calibration (error < 10%)\")\n",
    "else:\n",
    "    print(\"Result: POOR calibration (error >= 10%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exponential Decay Dataset\n",
    "\n",
    "Let's test GP on a different nonlinear function: exponential decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load exponential decay dataset\n",
    "dataset_exp = ExponentialDecayDataset(\n",
    "    n_samples=100,\n",
    "    noise_model='homoskedastic',\n",
    "    noise_level=0.05,\n",
    "    seed=42,\n",
    "    a=2.0,\n",
    "    b=3.0,\n",
    "    c=0.5\n",
    ")\n",
    "\n",
    "data_exp = dataset_exp.generate()\n",
    "\n",
    "print(f\"Dataset: {dataset_exp.get_function_form()}\")\n",
    "print(f\"Training samples: {len(data_exp.X_train)}\")\n",
    "\n",
    "# Prepare data\n",
    "X_train_exp = data_exp.X_train.reshape(-1, 1)\n",
    "y_train_exp = data_exp.y_train\n",
    "X_test_exp = data_exp.X_test.reshape(-1, 1)\n",
    "y_test_exp = data_exp.y_test\n",
    "\n",
    "# Fit GP\n",
    "kernel_exp = (\n",
    "    C(1.0, (1e-3, 1e3)) * RBF(0.1, (1e-3, 1.0)) +\n",
    "    WhiteKernel(0.01, (1e-5, 1e1))\n",
    ")\n",
    "\n",
    "gp_exp = GaussianProcessRegressor(\n",
    "    kernel=kernel_exp,\n",
    "    n_restarts_optimizer=10,\n",
    "    alpha=1e-10,\n",
    "    normalize_y=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nFitting GP to exponential decay data...\")\n",
    "gp_exp.fit(X_train_exp, y_train_exp)\n",
    "print(f\"Optimized kernel: {gp_exp.kernel_}\")\n",
    "\n",
    "# Predict\n",
    "y_pred_exp, y_std_exp = gp_exp.predict(X_plot, return_std=True)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_test_exp, y_std_test_exp = gp_exp.predict(X_test_exp, return_std=True)\n",
    "y_lower_test_exp = y_pred_test_exp - 1.96 * y_std_test_exp\n",
    "y_upper_test_exp = y_pred_test_exp + 1.96 * y_std_test_exp\n",
    "\n",
    "rmse_exp = np.sqrt(mean_squared_error(y_test_exp, y_pred_test_exp))\n",
    "coverage_exp = np.mean((y_test_exp >= y_lower_test_exp) & (y_test_exp <= y_upper_test_exp))\n",
    "width_exp = np.mean(y_upper_test_exp - y_lower_test_exp)\n",
    "\n",
    "print(f\"\\nRMSE: {rmse_exp:.4f}\")\n",
    "print(f\"Coverage: {coverage_exp:.3f}\")\n",
    "print(f\"Mean Width: {width_exp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot exponential decay results\n",
    "y_true_exp = dataset_exp._generate_clean(X_plot.flatten())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Uncertainty bands\n",
    "ax.fill_between(X_plot.flatten(), \n",
    "                 y_pred_exp - 1.96*y_std_exp, \n",
    "                 y_pred_exp + 1.96*y_std_exp,\n",
    "                 alpha=0.2, color='blue', label='95% CI')\n",
    "ax.fill_between(X_plot.flatten(), \n",
    "                 y_pred_exp - y_std_exp, \n",
    "                 y_pred_exp + y_std_exp,\n",
    "                 alpha=0.4, color='blue', label='68% CI')\n",
    "\n",
    "# Predictions\n",
    "ax.plot(X_plot, y_pred_exp, 'b-', linewidth=2.5, label='GP Mean', zorder=5)\n",
    "ax.plot(X_plot, y_true_exp, 'k--', linewidth=2, label='True Function', zorder=6)\n",
    "\n",
    "# Training data\n",
    "ax.scatter(X_train_exp, y_train_exp, c='red', s=70, alpha=0.8, \n",
    "           edgecolors='black', linewidth=1, label='Training Data', zorder=10)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title(f'GP on Exponential Decay: y = {dataset_exp.a}*exp(-{dataset_exp.b}*x) + {dataset_exp.c}\\n'\n",
    "             f'Coverage: {coverage_exp:.3f}, RMSE: {rmse_exp:.4f}', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Gaussian Processes Basics**:\n",
    "   - GPs are distributions over functions\n",
    "   - Defined by mean and covariance (kernel) functions\n",
    "   - Provide closed-form posterior predictions\n",
    "\n",
    "2. **Kernel Selection**:\n",
    "   - **RBF**: Very smooth, infinitely differentiable\n",
    "   - **Matern**: Controlled smoothness via $\\nu$ parameter\n",
    "   - **White Kernel**: Models observation noise\n",
    "   - Hyperparameters optimized via marginal likelihood\n",
    "\n",
    "3. **Uncertainty Quantification**:\n",
    "   - Predictive mean: $\\mu(x_*)$\n",
    "   - Predictive std: $\\sigma(x_*)$\n",
    "   - 95% prediction interval: $[\\mu - 1.96\\sigma, \\mu + 1.96\\sigma]$\n",
    "\n",
    "4. **Interpolation vs Extrapolation**:\n",
    "   - Low uncertainty near training data (interpolation)\n",
    "   - High uncertainty far from training data (extrapolation)\n",
    "   - GP provides honest uncertainty estimates\n",
    "\n",
    "5. **Calibration**:\n",
    "   - Well-calibrated GPs: observed coverage ≈ nominal coverage\n",
    "   - Calibration curves validate prediction intervals\n",
    "\n",
    "6. **Noise Models**:\n",
    "   - Standard GP assumes homoskedastic noise\n",
    "   - For heteroskedastic noise, use input-dependent noise models\n",
    "\n",
    "### Advantages of GPs\n",
    "\n",
    "- **Non-parametric**: No fixed functional form\n",
    "- **Bayesian**: Natural uncertainty quantification\n",
    "- **Flexible**: Kernel choice encodes prior knowledge\n",
    "- **Calibrated**: Well-calibrated uncertainty estimates\n",
    "- **Interpretable**: Clear physical meaning of hyperparameters\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Computational cost**: $O(n^3)$ scaling for $n$ training points\n",
    "- **Kernel choice**: Performance depends on kernel selection\n",
    "- **High dimensions**: Curse of dimensionality (length scale per dimension)\n",
    "- **Stationarity**: Standard kernels assume stationary covariance\n",
    "\n",
    "### When to Use GPs\n",
    "\n",
    "- Small to medium datasets ($n < 10,000$)\n",
    "- When uncertainty quantification is critical\n",
    "- Smooth functions with well-defined lengthscales\n",
    "- When prior knowledge about smoothness is available\n",
    "- Sequential decision-making (Bayesian optimization)\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- Rasmussen & Williams (2006). *Gaussian Processes for Machine Learning*\n",
    "- Murphy (2012). *Machine Learning: A Probabilistic Perspective*, Chapter 15\n",
    "- scikit-learn GP tutorial: https://scikit-learn.org/stable/modules/gaussian_process.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "summary_data = {\n",
    "    'Dataset': ['Gaussian (Homo)', 'Gaussian (Hetero)', 'Exponential Decay'],\n",
    "    'Kernel': ['RBF + White', 'RBF + White', 'RBF + White'],\n",
    "    'Coverage': [coverage, gp_noise_results['Homoskedastic']['coverage'], coverage_exp],\n",
    "    'RMSE': [rmse, np.nan, rmse_exp],\n",
    "    'Mean Width': [mean_width, np.nan, width_exp]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: GP REGRESSION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAll models achieve good coverage close to the target 0.95!\")\n",
    "print(\"This demonstrates the effectiveness of GP for uncertainty quantification.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
